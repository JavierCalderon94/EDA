{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "datos_streamers = pd.read_csv(\"1000_Streamers_2.csv\")\n",
    "nombre = list(datos_streamers.iloc[:,0].values)\n",
    "\n",
    "lista_fallos=('WarThunder_eSports','thek4sen','genesisgg','ow_esports__','MORGENSHTERN','Codonysus','JatyLito','Gustavin017','1GO_Casino','FRG_SR','Twitchmedia18','FishAtAce','湊あくあ_本物','rayasianboy','1_GO_Casino','bruxaofc01','FRG_stm','PimentoIa','FargoBossx','Twitchmedia13','ForexNowTV','Sabeky','twitchmedia54','twitchmedia6','xbuyeroficial','xFibii','Luxy_grlx','Luxury_qq','twitchmedia5','luchogal_10','Luxy_grl','FRG_FFS','KarmineCorp','Ilytog','LuxuryGirl_STRM','FRG_gj','BUXEXA_AO_VIVO','SlotHustla','Luxry_Fly','JakuLak','FRGxy')\n",
    "\n",
    "for i in lista_fallos:\n",
    "    nombre.remove(i)\n",
    "\n",
    "dict={\"streamer\":[],\n",
    "      \"fecha\":[],\n",
    "     \"duracion\":[],\n",
    "     \"media viewers\":[],\n",
    "     \"max viewers\":[],\n",
    "     \"followers\":[],\n",
    "     \"contenido 1\":[],\n",
    "     \"contenido 2\":[],\n",
    "     \"contenido 3\":[],\n",
    "     \"contenido 4\":[]}\n",
    "\n",
    "lista_fallos=[]\n",
    "\n",
    "for num, streamer in enumerate(nombre):\n",
    "\n",
    "    streamerb=[]  \n",
    "    fecha=[]\n",
    "    duracion=[]\n",
    "    media_viewers=[]\n",
    "    max_viewers=[]\n",
    "    followers=[]\n",
    "    contenido_1=[]\n",
    "    contenido_2=[]\n",
    "    contenido_3=[]\n",
    "    contenido_4=[]\n",
    "\n",
    "\n",
    "    try:\n",
    "        service = Service(executable_path='./chromedriver.exe')\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        url = \"https://twitchtracker.com\"+datos_streamers.iloc[num,1]+\"/streams\"\n",
    "\n",
    "        print(\"Obteniendo datos de:\",streamer)\n",
    "        driver.get(url)\n",
    "\n",
    "        personaldata = driver.find_element(By.XPATH, '/html/body/div[4]/div[2]/div[1]/div[3]/div[2]/button[1]/p')\n",
    "        personaldata.click()\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        pares = []\n",
    "        impares = []\n",
    "        contador = 0\n",
    "\n",
    "        for i in soup.find_all(\"tr\", class_=\"even\"):\n",
    "            for h in i.find_all(\"span\"):\n",
    "                if h.get_text()[0]==\"+\":\n",
    "                    continue\n",
    "                else:\n",
    "                    pares.append(h)\n",
    "\n",
    "        for i in soup.find_all(\"tr\", class_=\"odd\"):\n",
    "            for h in i.find_all(\"span\"):\n",
    "                if h.get_text()[0]==\"+\":\n",
    "                    continue\n",
    "                else:\n",
    "                    impares.append(h)\n",
    "\n",
    "        for h,j in enumerate(impares):\n",
    "            \n",
    "            contador+=1\n",
    "            match contador:\n",
    "                case 1:\n",
    "                    fecha.append(j.get_text())\n",
    "                    fecha.append(pares[h].get_text())\n",
    "                case 2:\n",
    "                    duracion.append(j.get_text())\n",
    "                    duracion.append(pares[h].get_text())\n",
    "                case 3:\n",
    "                    media_viewers.append(j.get_text())\n",
    "                    media_viewers.append(pares[h].get_text())\n",
    "                case 4:\n",
    "                    max_viewers.append(j.get_text())\n",
    "                    max_viewers.append(pares[h].get_text())\n",
    "                case 5:\n",
    "                    followers.append(j.get_text())\n",
    "                    followers.append(pares[h].get_text())\n",
    "                case 6:\n",
    "                    contador=0\n",
    "\n",
    "        contador = 0\n",
    "        for i in soup.find_all(\"td\", class_=\"games hidden-xs hidden-sm\"):\n",
    "\n",
    "            match contador:\n",
    "                case 1: \n",
    "                    contenido_2.append(pd.NA)\n",
    "                    contenido_3.append(pd.NA)\n",
    "                    contenido_4.append(pd.NA)\n",
    "                case 2: \n",
    "                    contenido_3.append(pd.NA)\n",
    "                    contenido_4.append(pd.NA)\n",
    "                case 3: \n",
    "                    contenido_4.append(pd.NA) \n",
    "\n",
    "            contador = 0      \n",
    "\n",
    "            for h, j in enumerate(i.find_all(\"img\")):\n",
    "                contador+=1\n",
    "                \n",
    "                match h:\n",
    "                    case 0:\n",
    "                        contenido_1.append(j.get(\"data-original-title\"))\n",
    "                    case 1:\n",
    "                        contenido_2.append(j.get(\"data-original-title\"))\n",
    "                    case 2:\n",
    "                        contenido_3.append(j.get(\"data-original-title\"))\n",
    "                    case 3:\n",
    "                        contenido_4.append(j.get(\"data-original-title\"))\n",
    "\n",
    "        match contador:\n",
    "            case 1: \n",
    "                contenido_2.append(pd.NA)\n",
    "                contenido_3.append(pd.NA)\n",
    "                contenido_4.append(pd.NA)\n",
    "            case 2: \n",
    "                contenido_3.append(pd.NA)\n",
    "                contenido_4.append(pd.NA)\n",
    "            case 3: \n",
    "                contenido_4.append(pd.NA)\n",
    "\n",
    "        #--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        if (len(pares)+len(impares))==120:\n",
    "            while soup!=1:\n",
    "                try:\n",
    "                    pagina_2 = driver.find_element(By.XPATH,'/html/body/div[2]/div[4]/div[5]/div[2]/div/ul/li[2]/a')\n",
    "                    pagina_2.click()\n",
    "                    break\n",
    "                except:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            pares = []\n",
    "            impares = []\n",
    "            contador = 0\n",
    "\n",
    "            for i in soup.find_all(\"tr\", class_=\"even\"):\n",
    "                for h in i.find_all(\"span\"):\n",
    "                    if h.get_text()[0]==\"+\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        pares.append(h)\n",
    "\n",
    "            for i in soup.find_all(\"tr\", class_=\"odd\"):\n",
    "                for h in i.find_all(\"span\"):\n",
    "                    if h.get_text()[0]==\"+\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        impares.append(h)\n",
    "\n",
    "            for h,j in enumerate(impares):\n",
    "                contador+=1\n",
    "                match contador:\n",
    "                    case 1:\n",
    "                        fecha.append(j.get_text())\n",
    "                        fecha.append(pares[h].get_text())\n",
    "                    case 2:\n",
    "                        duracion.append(j.get_text())\n",
    "                        duracion.append(pares[h].get_text())\n",
    "                    case 3:\n",
    "                        media_viewers.append(j.get_text())\n",
    "                        media_viewers.append(pares[h].get_text())\n",
    "                    case 4:\n",
    "                        max_viewers.append(j.get_text())\n",
    "                        max_viewers.append(pares[h].get_text())\n",
    "                    case 5:\n",
    "                        followers.append(j.get_text())\n",
    "                        followers.append(pares[h].get_text())\n",
    "                    case 6:\n",
    "                        contador=0\n",
    "\n",
    "            contador = 0\n",
    "            for i in soup.find_all(\"td\", class_=\"games hidden-xs hidden-sm\"):\n",
    "\n",
    "                match contador:\n",
    "                    case 1: \n",
    "                        contenido_2.append(pd.NA)\n",
    "                        contenido_3.append(pd.NA)\n",
    "                        contenido_4.append(pd.NA)\n",
    "                    case 2: \n",
    "                        contenido_3.append(pd.NA)\n",
    "                        contenido_4.append(pd.NA)\n",
    "                    case 3: \n",
    "                        contenido_4.append(pd.NA) \n",
    "\n",
    "                contador = 0      \n",
    "\n",
    "                for h, j in enumerate(i.find_all(\"img\")):\n",
    "                    contador+=1\n",
    "                    \n",
    "                    match h:\n",
    "                        case 0:\n",
    "                            contenido_1.append(j.get(\"data-original-title\"))\n",
    "                        case 1:\n",
    "                            contenido_2.append(j.get(\"data-original-title\"))\n",
    "                        case 2:\n",
    "                            contenido_3.append(j.get(\"data-original-title\"))\n",
    "                        case 3:\n",
    "                            contenido_4.append(j.get(\"data-original-title\"))\n",
    "\n",
    "            match contador:\n",
    "                case 1: \n",
    "                    contenido_2.append(pd.NA)\n",
    "                    contenido_3.append(pd.NA)\n",
    "                    contenido_4.append(pd.NA)\n",
    "                case 2: \n",
    "                    contenido_2.append(pd.NA)\n",
    "                    contenido_3.append(pd.NA)\n",
    "                case 3: \n",
    "                    contenido_4.append(pd.NA)\n",
    "\n",
    "            driver.close()\n",
    "\n",
    "        resta = len(fecha)-len(streamerb)\n",
    "        for i in list(range(resta)):\n",
    "            streamerb.append(streamer)\n",
    "\n",
    "        dict[\"streamer\"] += streamerb\n",
    "        dict[\"fecha\"] += fecha\n",
    "        dict[\"duracion\"] += duracion\n",
    "        dict[\"media viewers\"] += media_viewers\n",
    "        dict[\"max viewers\"] += max_viewers\n",
    "        dict[\"followers\"] += followers\n",
    "        dict[\"contenido 1\"] += contenido_1\n",
    "        dict[\"contenido 2\"] += contenido_2\n",
    "        dict[\"contenido 3\"] += contenido_3\n",
    "        dict[\"contenido 4\"] += contenido_4\n",
    "\n",
    "        time.sleep(random.randint(3,6))\n",
    "\n",
    "    except:\n",
    "        print(\"fallo\")\n",
    "        lista_fallos.append(streamer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
